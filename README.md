# Ollama_RAG
# Basic RAG Implementation using LangChain

# RAG Introduction using Ollama & LangChain

This project demonstrates a **basic Retrieval-Augmented Generation (RAG)** pipeline using a **local LLM (LLaMA 3.1)** via **Ollama**, along with **LangChain** and **FAISS** for vector storage.

The system retrieves relevant information from local documents and generates answers grounded in that data.

---

## üöÄ Features

- Local LLM execution using **Ollama**
- Document loading from text files
- Text chunking and embeddings
- Semantic search using **FAISS**
- RAG-based question answering
- No cloud or API keys required

---

## üõ†Ô∏è Tech Stack

- **Python** 3.10+
- **Ollama**
- **LLaMA 3.1**
- **LangChain**
- **FAISS**
- **Virtual Environment (venv)**

---

